{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kejeon/in_dev_RN20Q/blob/main/5_sweep_resent20_q_with_KD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tN1QuOTt7P5o"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "IN_LOCAL = not IN_COLAB\n",
        "USE_GITHUB = True\n",
        "USE_DRIVE = False\n",
        "device = 'cuda'\n",
        "\n",
        "assert not (USE_GITHUB and USE_DRIVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Du6bTmkysNv",
        "outputId": "aa46db43-f983-4c11-ab2b-f5572757b869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'in_dev_RN20Q'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 120 (delta 59), reused 77 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (120/120), 2.04 MiB | 4.51 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n",
            "/content/in_dev_RN20Q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "  !pip install wandb -qU\n",
        "  from google.colab import runtime\n",
        "  if USE_GITHUB:\n",
        "    !git clone https://github.com/kejeon/in_dev_RN20Q.git\n",
        "    %cd '/content/in_dev_RN20Q'\n",
        "  elif USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    %cd '/content/drive/MyDrive/GitHub/in_dev_RN20Q'\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "from model.resnet4c10q import ResNet20_Q\n",
        "from model.resnet4c10 import resnet20\n",
        "from mylib.KDResNetTrainer import ResNetTrainer\n",
        "\n",
        "wandb.login(key='e0c11d3ff2bee4c8775ba05863038fdac671c043')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ge8fxOob7P5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f4dec5-721d-4a54-9526-82b492cb5f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        }
      ],
      "source": [
        "api = wandb.Api()\n",
        "artifact = api.artifact('jke1994/ResNet20/model:v179')\n",
        "artifact.download(root='./pretrained_model')\n",
        "state_dict = torch.load('./pretrained_model/ckpt.pth', map_location=torch.device(device))\n",
        "\n",
        "teacher_model = resnet20()\n",
        "\n",
        "try:\n",
        "  teacher_model.load_state_dict(state_dict['net'])\n",
        "except:\n",
        "  # if the model is wrapped in a module, update all keys in state_dict to remove module.\n",
        "  state_dict['net'] = {k.replace('module.', ''): v for k, v in state_dict['net'].items()}\n",
        "  teacher_model.load_state_dict(state_dict['net'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bcKFZkeC7P5s"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    run = wandb.init()\n",
        "\n",
        "    arch_tag = \"ResNet20_Q\"\n",
        "    batch_size = 128\n",
        "    dataset = \"CIFAR10\"\n",
        "    T=wandb.config.T\n",
        "    alpha=wandb.config.alpha\n",
        "    lr=0.012\n",
        "\n",
        "    student_model = ResNet20_Q(a_bit=4, w_bit=1)\n",
        "\n",
        "    my_trainer = ResNetTrainer(dataset=dataset,\n",
        "                           arch_tag=arch_tag,\n",
        "                           student_model=student_model,\n",
        "                           teacher_model=teacher_model,\n",
        "                           T=T,\n",
        "                           alpha=alpha,\n",
        "                           device=device,\n",
        "                           batch_size=batch_size,\n",
        "                           lr=lr)\n",
        "\n",
        "    my_trainer.train_script(200)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8aUtzyQ7P5t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9a17bcfc-f57f-48ef-b610-9ec4fdeb5160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ul7ggdnb with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tT: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjke1994\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/in_dev_RN20Q/wandb/run-20230718_080436-ul7ggdnb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jke1994/my-first-sweep/runs/ul7ggdnb' target=\"_blank\">winter-sweep-4</a></strong> to <a href='https://wandb.ai/jke1994/my-first-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jke1994/my-first-sweep/sweeps/qkr840ok' target=\"_blank\">https://wandb.ai/jke1994/my-first-sweep/sweeps/qkr840ok</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jke1994/my-first-sweep' target=\"_blank\">https://wandb.ai/jke1994/my-first-sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/jke1994/my-first-sweep/sweeps/qkr840ok' target=\"_blank\">https://wandb.ai/jke1994/my-first-sweep/sweeps/qkr840ok</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jke1994/my-first-sweep/runs/ul7ggdnb' target=\"_blank\">https://wandb.ai/jke1994/my-first-sweep/runs/ul7ggdnb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 28440354.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "\n",
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:   0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n",
            "Epoch 0: 100%|██████████| 391/391 [00:49<00:00,  7.92it/s, train_acc=0.187, xentropy=2.146]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        }
      ],
      "source": [
        "wandb.agent(sweep_id='qkr840ok', function=train, count=5, project='my-first-sweep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLxQTnCV7P5t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}